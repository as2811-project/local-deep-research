{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from pydantic import BaseModel\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ],
   "id": "1fc9c830090d20ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Queries(BaseModel):\n",
    "    wide_queries : list[str]\n",
    "    deep_queries : list[str]\n",
    "\n",
    "def generate_queries(topic, width, depth, deepdive_topic):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=f\"You are an expert in topic:{topic}. Your task is to generate search queries to aid the user's research on said topic. The user will provide you research width and depth. Width indicates how wide the research needs to be. Depth indicates how deep the research needs to be for a specific topic. You need to generate {width} search queries to cover the width of the research and {depth} search queries to go deeper into the subtopic: {deepdive_topic}.\",\n",
    "        config={\n",
    "            'response_mime_type':'application/json',\n",
    "            'response_schema':list[Queries],\n",
    "        }\n",
    "    )\n",
    "    return json.loads(response.text)"
   ],
   "id": "26abe9a54b169bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_context(search_query:str):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=f\"{search_query}\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[types.Tool(\n",
    "                google_search=types.GoogleSearchRetrieval\n",
    "            )]\n",
    "        )\n",
    "    )\n",
    "    return response.text, response.candidates[0].grounding_metadata.grounding_chunks"
   ],
   "id": "ac3991987e04a83b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_report():\n",
    "    \"\"\"Main function for generating a report.\"\"\"\n",
    "    topic = input(\"Enter the topic: \")\n",
    "    width = input(\"Enter the width: \")\n",
    "    depth = input(\"Enter the depth: \")\n",
    "    deepdive_topic = input(\"Enter the topic you want to dive deep into: \")\n",
    "    \n",
    "    sys_instruct = f\"You are an expert analyst in the topic: {topic}. You've been given a lot of context (which you produced earlier) here supporting the user's research on said topic. With this information, generate a detailed (1000 words) report as instructed. Be sure to include all sources, persons, objects etc in the report. Additionally, you must dive deeper into {deepdive_topic} as that is what the user would like to dive deeper into. You MUST include a references section and appropriately add citations. Use the Citations object provided to you, each citation has a title and a URI, the citations in the report MUST be hyperlinked with the corresponding uri so that the user can follow it if necessary. Ensure that the report itself adheres to the user's requirements and does not deviate away from the research's goals. Feel free to provide tables if required.\"\n",
    "    \n",
    "    search_queries = generate_queries(topic, width, depth, deepdive_topic)\n",
    "    total_context = []\n",
    "    sources = []\n",
    "    \n",
    "    for query_type in [\"wide_queries\", \"deep_queries\"]:\n",
    "        for query in search_queries[0][query_type]:\n",
    "            context, source_list = generate_context(query)\n",
    "            total_context.append(context)\n",
    "            for src in source_list:\n",
    "                print(\"Retrieved context from \", src.web.title)\n",
    "                sources.append({\"title\": src.web.title, \"uri\": src.web.uri})\n",
    "                \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=sys_instruct\n",
    "        ),\n",
    "        contents=f\"Context: {json.dumps(total_context)} Citations: {json.dumps(sources)}\"\n",
    "    )\n",
    "    return response.text\n"
   ],
   "id": "ebd886d4c39139aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    final_report = generate_report()\n",
    "    display(Markdown(final_report))"
   ],
   "id": "f10997afebb66584",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
